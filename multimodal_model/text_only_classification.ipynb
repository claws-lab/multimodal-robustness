{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJQh2cy6f-_X"
   },
   "outputs": [],
   "source": [
    "!pip install tweet-preprocessor\n",
    "!pip install transformers\n",
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfOvbllZorfJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive._mount('/content/drive/', force_remount=True)\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/ColabFiles/MultimodalLanguageDisparity')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87P6TZVazfVK"
   },
   "outputs": [],
   "source": [
    "#!pip install torch==1.4.0\n",
    "import torch\n",
    "# torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5B2mxdSJ14mD"
   },
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95rQDJ7kik3n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import preprocessor as tp\n",
    "import ftfy\n",
    "\n",
    "def FormatText(filename):\n",
    "    data_list = []\n",
    "    data = pd.read_csv(filename, sep = '\\t')\n",
    "\n",
    "    # Clean the \"tweet_text\" column\n",
    "    tp.set_options(tp.OPT.URL, tp.OPT.EMOJI, tp.OPT.SMILEY, tp.OPT.RESERVED)\n",
    "    data[\"tweet_text\"] = data[\"tweet_text\"].apply(lambda x: tp.clean(x))\n",
    "    data[\"tweet_text\"] = data[\"tweet_text\"].apply(lambda x : ftfy.fix_text(x))\n",
    "    data[\"tweet_text\"] = data[\"tweet_text\"].str.replace(r'\\\\n',' ', regex=True) \n",
    "    data[\"tweet_text\"] = data[\"tweet_text\"].str.replace(r\"\\'t\", \" not\")\n",
    "    data[\"tweet_text\"] = data[\"tweet_text\"].str.strip()\n",
    "    data[\"tweet_text\"] = data[\"tweet_text\"].str.replace(\"#\",\"\")\n",
    "    data[\"tweet_text\"] = data[\"tweet_text\"].str.replace(\"@\",\"\")\n",
    "    tweet_id = data['tweet_id'].to_list()\n",
    "    image_id = data['image_id'].to_list()\n",
    "    tweet_text = data['tweet_text'].to_list()\n",
    "    tweet_text = [str(x) for x in tweet_text]\n",
    "\n",
    "    label = data['label'].to_list()\n",
    "    alignment = data['label_text_image'].to_list()\n",
    "    for a_var in range(len(tweet_id)):\n",
    "        data_point = {}\n",
    "        if alignment[a_var] == 'Positive':\n",
    "            data_point['tweet_id'] = tweet_id[a_var]\n",
    "            data_point['image_id'] = image_id[a_var]\n",
    "            data_point['tweet_text'] = tweet_text[a_var].lower()\n",
    "            data_point['label'] = label[a_var]\n",
    "            data_list.append(data_point)\n",
    "    return data_list\n",
    "\n",
    "folderpath = './'\n",
    "!pwd\n",
    "filenames = ['task_humanitarian_text_img_dev.tsv', 'task_humanitarian_text_img_test.tsv', 'task_humanitarian_text_img_train.tsv']\n",
    "\n",
    "for a_file in filenames:\n",
    "    data = FormatText(folderpath + a_file)\n",
    "    source_text = [x['tweet_text'] for x in data]\n",
    "    for a_var in range(len(data)):\n",
    "        data[a_var]['tweet_text'] = [source_text[a_var]]\n",
    "    with open(folderpath + a_file.split('.')[0] + '.pkl', 'wb') as f:\n",
    "        pkl.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hb3wL8xI1dXe"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import os, sys, re\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "NUM_LABELS = 5\n",
    "\n",
    "class DatasetFormatting(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def appendCaptions(data):\n",
    "  with open('final_captions_all.pkl', 'rb') as f:\n",
    "    captions = pkl.load(f)\n",
    "  for a_point in data:\n",
    "    try:\n",
    "      a_point['tweet_text'][0] = a_point['tweet_text'][0] + ' ' + captions[a_point['image_id'] + '.jpg'].split('<br>')[0]\n",
    "    except KeyError:\n",
    "      a_point['tweet_text'][0] = a_point['tweet_text'][0] + ' ' + captions[a_point['image_id'] + '.png'].split('<br>')[0]\n",
    "  return data\n",
    "\n",
    "def AppendText(data):\n",
    "  with open('./image_text_kw,pkl', 'rb') as f: # THIS IS FOR EVAL OF CLOSES IMAGE'S CAPTION\n",
    "    extra_text = pkl.load(f)\n",
    "\n",
    "  for a_point in data:    # THIS IS FOR EVAL OF RULE-BASED MODELS \n",
    "    if str(a_point['tweet_id']) in [str(x['tweet_id']) for x in extra_text]:\n",
    "        for elt in extra_text:\n",
    "            if str(elt['tweet_id']) == str(a_point['tweet_id']):\n",
    "                a_point['tweet_text'][0] = a_point['tweet_text'][0] + ' ' + elt['text']\n",
    "  return data\n",
    "\n",
    "def AlignFormatLabels(list, labels):\n",
    "    aligned_list = []\n",
    "    for item in list:\n",
    "        if item == 'vehicle_damage':\n",
    "            aligned_list.append('infrastructure_and_utility_damage')\n",
    "        elif item == 'missing_or_found_people' or item == 'injured_or_dead_people':\n",
    "            aligned_list.append('affected_individuals')\n",
    "        else:\n",
    "            aligned_list.append(item)\n",
    "    final_labels = []\n",
    "    for item in aligned_list:\n",
    "        final_labels.append(labels.index(item))\n",
    "    return final_labels\n",
    "\n",
    "def GetData(folder):\n",
    "    filenames_prefix = 'task_humanitarian_text_img_'\n",
    "    with open(folder + filenames_prefix + 'train.pkl', 'rb') as f:\n",
    "        train_data = pkl.load(f)\n",
    "    with open(folder + filenames_prefix + 'dev.pkl', 'rb') as f:\n",
    "        val_data = pkl.load(f)\n",
    "    with open(folder + filenames_prefix + 'test.pkl', 'rb') as f:\n",
    "        test_data = pkl.load(f)\n",
    "     \n",
    "    print(train_data[0])\n",
    "    print(val_data[0])\n",
    "    print(test_data[0])\n",
    "\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    train_ids = []\n",
    "    for a_point in train_data:\n",
    "        # print(a_point)\n",
    "        train_ids.append(a_point['tweet_id'])\n",
    "        train_texts.append(a_point['tweet_text'])\n",
    "        train_labels.append(a_point['label'])\n",
    "    val_texts = []\n",
    "    val_labels = []\n",
    "    val_ids = []\n",
    "    for a_point in val_data:\n",
    "        val_ids.append(a_point['tweet_id'])\n",
    "        val_texts.append(a_point['tweet_text'])\n",
    "        val_labels.append(a_point['label'])\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    test_ids = []\n",
    "    for a_point in test_data:\n",
    "        test_ids.append(a_point['tweet_id'])\n",
    "        test_texts.append(a_point['tweet_text'])\n",
    "        test_labels.append(a_point['label'])\n",
    "    my_labels = ['affected_individuals', 'infrastructure_and_utility_damage', 'not_humanitarian', 'other_relevant_information', 'rescue_volunteering_or_donation_effort']\n",
    "    \n",
    "    train_texts = [x[0] for x in train_texts]\n",
    "    val_texts = [x[0] for x in val_texts]\n",
    "    test_texts = [x[0] for x in test_texts]\n",
    "    train_labels = AlignFormatLabels(train_labels, my_labels)\n",
    "    val_labels = AlignFormatLabels(val_labels, my_labels)\n",
    "    test_labels = AlignFormatLabels(test_labels, my_labels)\n",
    "    return train_texts, train_labels, train_ids, val_texts, val_labels, val_ids, test_texts, test_labels, test_ids\n",
    "\n",
    "pickle_folder_path = './'\n",
    "train_texts, train_labels, train_ids, val_texts, val_labels, val_ids, test_texts, test_labels, test_ids = GetData(pickle_folder_path)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = DatasetFormatting(train_encodings, train_labels)\n",
    "val_dataset = DatasetFormatting(val_encodings, val_labels)\n",
    "test_dataset = DatasetFormatting(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nz5bq_8mTRpL"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions[0].argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE9uLDts2X5Z"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_',          # output directory\n",
    "    num_train_epochs=5 ,             # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batchx size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    learning_rate = 5e-5\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    save_steps = 500,\n",
    "    evaluation_strategy='steps'\n",
    ")\n",
    "\n",
    "# Here you can specify the BERT model you want to use to train the text-only classifier\n",
    "# As an additional consideration, you want to use the same BERT model configs as those you use for training the POINTER model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=NUM_LABELS)\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g-yddHcyOf2"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('finetuned_bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e7AHcFdI9wG"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "device = \"cuda:0\"\n",
    "model = model.to(device)\n",
    "\n",
    "embeddings_train = {}\n",
    "embeddings_val = {}\n",
    "embeddings_test = {}\n",
    "for i in tqdm(range(0,len(train_ids))):\n",
    "    input = torch.tensor(tokenizer.encode(train_texts[i])).unsqueeze(0).to(device)\n",
    "    outputs = model(input)\n",
    "    embedding = torch.mean(outputs.hidden_states[-1], 1, True).cpu()\n",
    "    embedding = embedding.detach().numpy()\n",
    "    embedding = np.reshape(embedding, (embedding.shape[0], embedding.shape[-1])) \n",
    "    embeddings_train[train_ids[i]] = embedding\n",
    "\n",
    "for i in tqdm(range(0,len(val_ids))):\n",
    "    input = torch.tensor(tokenizer.encode(val_texts[i])).unsqueeze(0).to(device)\n",
    "    outputs = model(input)\n",
    "    embedding = torch.mean(outputs.hidden_states[-1], 1, True).cpu()\n",
    "    embedding = embedding.detach().numpy()\n",
    "    embedding = np.reshape(embedding, (embedding.shape[0], embedding.shape[-1])) \n",
    "    embeddings_val[val_ids[i]] = embedding\n",
    "\n",
    "for i in tqdm(range(0,len(test_ids))):\n",
    "    input = torch.tensor(tokenizer.encode(test_texts[i])).unsqueeze(0).to(device)\n",
    "    outputs = model(input)\n",
    "    embedding = torch.mean(outputs.hidden_states[-1], 1, True).cpu()\n",
    "    embedding = embedding.detach().numpy()\n",
    "    embedding = np.reshape(embedding, (embedding.shape[0], embedding.shape[-1])) \n",
    "    embeddings_test[test_ids[i]] = embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahZf5EpTJEfZ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./embeddings_train_crisis.pickle', 'wb') as handle:\n",
    "    pickle.dump(embeddings_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./embeddings_val_crisis.pickle', 'wb') as handle:\n",
    "    pickle.dump(embeddings_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./embeddings_test_crisis.pickle', 'wb') as handle:\n",
    "    pickle.dump(embeddings_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vs7DNnr74rsc"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./aaa_embeddings_train_crisis.pickle', 'rb') as handle:\n",
    "    embeddings_train_dict = pickle.load(handle)\n",
    "with open('./aaa_embeddings_val_crisis.pickle', 'rb') as handle:\n",
    "    embeddings_val_dict = pickle.load(handle)\n",
    "with open('./aaa_embeddings_test_crisis.pickle', 'rb') as handle:\n",
    "    embeddings_test_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThVt7NGYH_HC"
   },
   "outputs": [],
   "source": [
    "len(set(embeddings_val_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fop6j5XdP9bn"
   },
   "outputs": [],
   "source": [
    "train_text_emb, train_text_id = [], []\n",
    "for tweet_id in embeddings_train_dict.keys():\n",
    "\ttrain_text_id.append(tweet_id)\n",
    "\ttrain_text_emb.append(embeddings_train_dict[tweet_id])\n",
    "test_text_emb, test_text_id = [], []\n",
    "for tweet_id in embeddings_test_dict.keys():\n",
    "\ttest_text_id.append(tweet_id)\n",
    "\ttest_text_emb.append(embeddings_test_dict[tweet_id])\n",
    "val_text_emb, val_text_id = [], []\n",
    "for tweet_id in embeddings_val_dict.keys():\n",
    "\tval_text_id.append(tweet_id)\n",
    "\tval_text_emb.append(embeddings_val_dict[tweet_id])\n",
    "train_text_emb = np.array(train_text_emb)\n",
    "train_text_emb = train_text_emb.reshape((train_text_emb.shape[0], train_text_emb.shape[-1]))\n",
    "test_text_emb = np.array(test_text_emb)\n",
    "test_text_emb = test_text_emb.reshape((test_text_emb.shape[0], test_text_emb.shape[-1]))\n",
    "val_text_emb = np.array(val_text_emb)\n",
    "val_text_emb = val_text_emb.reshape((val_text_emb.shape[0], val_text_emb.shape[-1]))\n",
    "print(train_text_emb.shape)\n",
    "print(train_text_id[:5])\n",
    "print(test_text_id[:5])\n",
    "print(val_text_id[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cxPZFhjJBZi"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "trainer.evaluate()\n",
    "#Testing\n",
    "probs, _, metrics = trainer.predict(test_dataset)\n",
    "pred_labels = probs[0].argmax(-1)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, pred_labels, average='macro')\n",
    "acc = accuracy_score(test_labels, pred_labels)\n",
    "test_dict = {'accuracy': acc,\n",
    "'f1': f1,\n",
    "'precision': precision,\n",
    "'recall': recall}\n",
    "print(test_dict)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6h4Ey6dzgpx"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('trained_models', local_files_only = True)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('trained_models', local_files_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfx0Kg1z7vEV"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "#Testing\n",
    "probs, _, metrics = model.predict(test_dataset)\n",
    "pred_labels = probs[0].argmax(-1)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, pred_labels, average='macro')\n",
    "acc = accuracy_score(test_labels, pred_labels)\n",
    "test_dict = {'accuracy': acc,\n",
    "'f1': f1,\n",
    "'precision': precision,\n",
    "'recall': recall}\n",
    "print(test_dict)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "text_only_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
